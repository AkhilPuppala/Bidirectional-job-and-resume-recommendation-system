{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Job - Pre-processing and Modelling Iteration 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries import\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import re\n",
    "import datetime\n",
    "from datetime import date\n",
    "from time import strptime\n",
    "\n",
    "import RAKE as rake\n",
    "import operator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######################################################################################\n",
    "\n",
    "# Working on Job description Text Data\n",
    "######################################################################################   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading my sorted job csv\n",
    "job = pd.read_csv('C:/Users/CHARAN SRI SAI/Downloads/rsjbrproj/data_gathering_eda/sorted_jobs_master_new2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 38941 entries, 0 to 38940\n",
      "Data columns (total 18 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   index             38941 non-null  int64  \n",
      " 1   company           38941 non-null  object \n",
      " 2   education         38941 non-null  object \n",
      " 3   experience        38941 non-null  int64  \n",
      " 4   industry          38941 non-null  object \n",
      " 5   jobdescription    38941 non-null  object \n",
      " 6   jobtitle          38941 non-null  object \n",
      " 7   payrate           38941 non-null  object \n",
      " 8   postdate          38908 non-null  object \n",
      " 9   skills            38941 non-null  object \n",
      " 10  experience_range  38941 non-null  int64  \n",
      " 11  Salary_range      38941 non-null  float64\n",
      " 12  is_grad           38941 non-null  float64\n",
      " 13  is_postgrad       38941 non-null  float64\n",
      " 14  is_doc            38941 non-null  float64\n",
      " 15  j_id              38941 non-null  int64  \n",
      " 16  location          38941 non-null  object \n",
      " 17  loc_name          35313 non-null  object \n",
      "dtypes: float64(4), int64(4), object(10)\n",
      "memory usage: 5.3+ MB\n"
     ]
    }
   ],
   "source": [
    "job.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###########################################################################################################################\n",
    "# Understanding Job_description column (using NLP)\n",
    "###########################################################################################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. NLP - NLTK application to understand most used words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\CHARAN SRI\n",
      "[nltk_data]     SAI\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Import all the dependencies\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "set(stopwords.words('english'))\n",
    "# nltk.download('abc')\n",
    "# from nltk.corpus import abc\n",
    "# from nltk import RegexpTokenizer\n",
    "\n",
    "import string\n",
    "stopwords = set(stopwords.words(\"english\"))\n",
    "import gensim\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining tokenizer \n",
    "def my_tokenizer(text):\n",
    "    # 1. split at whitespace\n",
    "    text = text.split(' ')\n",
    "    \n",
    "    #2. lowercase\n",
    "    text = [word.lower() for word in text]\n",
    "    \n",
    "    #3. Remove puncutation\n",
    "    #table to replace puncuation\n",
    "    punc_table = str.maketrans('','',string.punctuation)\n",
    "    \n",
    "    #call translate()\n",
    "    text = [word.translate(punc_table) for word in text]\n",
    "    \n",
    "    #4. remove stopwords\n",
    "    text = [word for word in text if word not in stopwords]\n",
    "    \n",
    "    #5. lemmmatize\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    text = [lemmatizer.lemmatize(word, pos='v') for word in text]\n",
    "    text = [lemmatizer.lemmatize(word, pos='n') for word in text]\n",
    "    text = [lemmatizer.lemmatize(word, pos='a') for word in text]\n",
    "    \n",
    "    #6. remove empty strings\n",
    "    text = [word for word in text if word !='']\n",
    "    \n",
    "    return text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CHARAN SRI SAI\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5415</th>\n",
       "      <td>job</td>\n",
       "      <td>140463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10756</th>\n",
       "      <td></td>\n",
       "      <td>87571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7596</th>\n",
       "      <td>profile</td>\n",
       "      <td>84883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3031</th>\n",
       "      <td>description</td>\n",
       "      <td>75494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2388</th>\n",
       "      <td>company</td>\n",
       "      <td>72167</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              word   count\n",
       "5415           job  140463\n",
       "10756            Â    87571\n",
       "7596       profile   84883\n",
       "3031   description   75494\n",
       "2388       company   72167"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#apply count vectorizor for tokenization\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# # # Now, time to count vectorize to get most used words in job description\n",
    "X_train = job['jobdescription']\n",
    "X_train.shape\n",
    "\n",
    "# 1. Instantiate\n",
    "bagofwords = CountVectorizer(min_df = 15, tokenizer = my_tokenizer)\n",
    "# 2. Fit\n",
    "bagofwords.fit(X_train)\n",
    "# 3. Transform\n",
    "X_train_p = bagofwords.transform(X_train)\n",
    "\n",
    "# analysing JD words in dataframe\n",
    "word_counts = np.array(np.sum(X_train_p, axis=0)).reshape((-1,))\n",
    "words = np.array(bagofwords.get_feature_names_out())\n",
    "words_df = pd.DataFrame({\"word\":words, \n",
    "                         \"count\":word_counts})\n",
    "words_rank = words_df.sort_values(by=\"count\", ascending=False)\n",
    "words_rank.to_csv('jd_words_rank_.csv') # Storing for inspection \n",
    "words_rank.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "barplot() takes from 0 to 1 positional arguments but 2 positional arguments (and 1 keyword-only argument) were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m12\u001b[39m,\u001b[38;5;241m8\u001b[39m))\n\u001b[0;32m      4\u001b[0m rk_w \u001b[38;5;241m=\u001b[39m words_rank[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcount\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m \u001b[43msns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbarplot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwords_rank\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mword\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwords_rank\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcount\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpalette\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolor_palette\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mYlOrRd\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTop 10 Most Common Words in Job Description\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      7\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[1;31mTypeError\u001b[0m: barplot() takes from 0 to 1 positional arguments but 2 positional arguments (and 1 keyword-only argument) were given"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualizing top 10 words\n",
    "import seaborn as sns\n",
    "plt.figure(figsize=(12,8))\n",
    "rk_w = words_rank['count'].astype(str)\n",
    "sns.barplot(words_rank['word'][:10], words_rank['count'][:10].astype(str), palette=sns.color_palette(\"YlOrRd\"))\n",
    "plt.title('Top 10 Most Common Words in Job Description')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. NLP - TF-IDF application to get a list of all tokens \n",
    "-- This helped me to gather what words needed to be in stop-words list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CHARAN SRI SAI\\AppData\\Local\\Temp\\ipykernel_7936\\3798363525.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_job_descriptions['jd_combo'] = job['jobtitle'] +\" \"+job['jobdescription']+ \" \"+job['skills'] + \" \" + job['industry']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>j_id</th>\n",
       "      <th>jobtitle</th>\n",
       "      <th>company</th>\n",
       "      <th>jd_combo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>walkin data entry operator (night shift)</td>\n",
       "      <td>MM Media Pvt Ltd</td>\n",
       "      <td>walkin data entry operator (night shift) Job D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>work based onhome based part time.</td>\n",
       "      <td>find live infotech</td>\n",
       "      <td>work based onhome based part time. Job Descrip...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>pl/sql developer - sql</td>\n",
       "      <td>Softtech Career Infosystem Pvt. Ltd</td>\n",
       "      <td>pl/sql developer - sql Job Description Â  Send ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>manager/ad/partner - indirect tax - ca</td>\n",
       "      <td>Onboard HRServices LLP</td>\n",
       "      <td>manager/ad/partner - indirect tax - ca Job Des...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>manager/ad/partner - indirect tax - ca</td>\n",
       "      <td>Onboard HRServices LLP</td>\n",
       "      <td>manager/ad/partner - indirect tax - ca Job Des...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   j_id                                  jobtitle  \\\n",
       "0     0  walkin data entry operator (night shift)   \n",
       "1     1        work based onhome based part time.   \n",
       "2     2                    pl/sql developer - sql   \n",
       "3     3    manager/ad/partner - indirect tax - ca   \n",
       "4     3    manager/ad/partner - indirect tax - ca   \n",
       "\n",
       "                               company  \\\n",
       "0                     MM Media Pvt Ltd   \n",
       "1                   find live infotech   \n",
       "2  Softtech Career Infosystem Pvt. Ltd   \n",
       "3               Onboard HRServices LLP   \n",
       "4               Onboard HRServices LLP   \n",
       "\n",
       "                                            jd_combo  \n",
       "0  walkin data entry operator (night shift) Job D...  \n",
       "1  work based onhome based part time. Job Descrip...  \n",
       "2  pl/sql developer - sql Job Description Â  Send ...  \n",
       "3  manager/ad/partner - indirect tax - ca Job Des...  \n",
       "4  manager/ad/partner - indirect tax - ca Job Des...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_job_descriptions = job[['j_id','jobtitle','company' ]]\n",
    "df_job_descriptions['jd_combo'] = job['jobtitle'] +\" \"+job['jobdescription']+ \" \"+job['skills'] + \" \" + job['industry']\n",
    "df_job_descriptions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CHARAN SRI SAI\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\feature_extraction\\text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['Ã«Å'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(38941, 58510)\n",
      "(38941, 4)\n"
     ]
    }
   ],
   "source": [
    "#apply tf-idf\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stopwords.append('Ã£Â¯Ã¦âÃ«Å')\n",
    "#Transforms words to TFIDF\n",
    "vectorizer = TfidfVectorizer(stop_words = stopwords)\n",
    "\n",
    "index = 0\n",
    "keys = {}\n",
    "\n",
    "for jd in df_job_descriptions.itertuples() :\n",
    "    key = jd[1]\n",
    "    keys[key] = index\n",
    "    index += 1\n",
    "\n",
    "#Fit the vectorizer to the data\n",
    "vectorizer.fit(df_job_descriptions['jd_combo'].fillna(''))\n",
    "\n",
    "#Transform the data\n",
    "tfidf_scores = vectorizer.transform(df_job_descriptions['jd_combo'].fillna(''))\n",
    "\n",
    "print(tfidf_scores.shape)\n",
    "print(df_job_descriptions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get all words \n",
    "import pandas as pd\n",
    "\n",
    "# Convert sparse TF-IDF scores to a sparse DataFrame\n",
    "test = pd.DataFrame.sparse.from_spmatrix(tfidf_scores, columns=vectorizer.get_feature_names_out())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>0000</th>\n",
       "      <th>00000</th>\n",
       "      <th>0000gmt</th>\n",
       "      <th>0001pt</th>\n",
       "      <th>00029</th>\n",
       "      <th>00034</th>\n",
       "      <th>000402</th>\n",
       "      <th>00053</th>\n",
       "      <th>...</th>\n",
       "      <th>Ã¯Æ</th>\n",
       "      <th>Ã¯ÆÂ¼</th>\n",
       "      <th>Ã¯ÆÅ¾</th>\n",
       "      <th>Å100</th>\n",
       "      <th>Åmost</th>\n",
       "      <th>Årecognition</th>\n",
       "      <th>Åto</th>\n",
       "      <th>Å¡Ã¢</th>\n",
       "      <th>Å¡Ã£</th>\n",
       "      <th>Å¾Ã¢</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.055441</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.065396</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã 58510 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    00       000  0000  00000  0000gmt  0001pt  00029  00034  000402  00053  \\\n",
       "0  0.0  0.055441   0.0    0.0      0.0     0.0    0.0    0.0     0.0    0.0   \n",
       "1  0.0  0.065396   0.0    0.0      0.0     0.0    0.0    0.0     0.0    0.0   \n",
       "2  0.0  0.000000   0.0    0.0      0.0     0.0    0.0    0.0     0.0    0.0   \n",
       "3  0.0  0.000000   0.0    0.0      0.0     0.0    0.0    0.0     0.0    0.0   \n",
       "4  0.0  0.000000   0.0    0.0      0.0     0.0    0.0    0.0     0.0    0.0   \n",
       "\n",
       "   ...   Ã¯Æ  Ã¯ÆÂ¼  Ã¯ÆÅ¾  Å100  Åmost  Årecognition  Åto   Å¡Ã¢   Å¡Ã£   Å¾Ã¢  \n",
       "0  ...  0.0  0.0  0.0   0.0    0.0           0.0  0.0  0.0  0.0  0.0  \n",
       "1  ...  0.0  0.0  0.0   0.0    0.0           0.0  0.0  0.0  0.0  0.0  \n",
       "2  ...  0.0  0.0  0.0   0.0    0.0           0.0  0.0  0.0  0.0  0.0  \n",
       "3  ...  0.0  0.0  0.0   0.0    0.0           0.0  0.0  0.0  0.0  0.0  \n",
       "4  ...  0.0  0.0  0.0   0.0    0.0           0.0  0.0  0.0  0.0  0.0  \n",
       "\n",
       "[5 rows x 58510 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Show\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As count vectorizer and Tf-Idf are only exploding my column numbers. It might not be wise to proceed with any of these. Moveover, I need to compare job description with Resume, that may not with fair comparison. So I will use these results so far for customizing stop word list. And will later use Doc2Vec to train my model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating my Stopword list \n",
    "\n",
    "### As seen there are so many unwanted tokens like numbers,Ã¯ÆÂ¼ etc , I need to add them in \"stop words\" list to train model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting list of all tokens\n",
    "word_list = test.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Getting a list of unwanted words as s_words and adding to stopwords\n",
    "s_words =[]\n",
    "for word in word_list:\n",
    "    #print(word)\n",
    "    if re.search(\"^\\W|^\\d\",word):\n",
    "        s_words.append(word)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_words.append('')        \n",
    "from nltk.corpus import stopwords\n",
    "stopword_set = set(stopwords.words('english'))\n",
    "stopword_set = list(stopword_set)\n",
    "stopword_set.extend(s_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting all text data for DOC2VEC modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    walkin data entry operator (night shift) Job D...\n",
       "1    work based onhome based part time. Job Descrip...\n",
       "2    pl/sql developer - sql Job Description Â  Send ...\n",
       "3    manager/ad/partner - indirect tax - ca Job Des...\n",
       "4    manager/ad/partner - indirect tax - ca Job Des...\n",
       "5    manager/ad/partner - indirect tax - ca Job Des...\n",
       "6    manager/ad/partner - indirect tax - ca Job Des...\n",
       "7    manager/ad/partner - indirect tax - ca Job Des...\n",
       "8    manager/ad/partner - indirect tax - ca Job Des...\n",
       "9    java technical lead (6-8 yrs) - Job Descriptio...\n",
       "Name: jd_combo, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using concatenated text columns\n",
    "docs = df_job_descriptions['jd_combo']\n",
    "docs_sample = docs.head(10)\n",
    "docs_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pre-processing with custom stop word list\n",
    "def preprocess(text):\n",
    "    stop_words = stopword_set\n",
    "    #0. split words by whitespace\n",
    "    text = text.split()\n",
    "    \n",
    "    \n",
    "    # 1. lower case\n",
    "    text = [word.lower() for word in text]\n",
    "    \n",
    "    # 2. remove punctuations\n",
    "    punc_table = str.maketrans('','',string.punctuation)\n",
    "    text = [word.translate(punc_table) for word in text]\n",
    "    \n",
    "    # 3. remove stop words\n",
    "    text = [word for word in text if word not in stop_words]\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calling my pre-process to tokenize \n",
    "tokenized_doc = []\n",
    "doc = df_job_descriptions['jd_combo']\n",
    "#doc = docs_sample\n",
    "for d in doc:\n",
    "    tokenized_doc.append(preprocess(d))\n",
    "#tokenized_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert tokenized document into gensim formated tagged data\n",
    "tagged_data = [TaggedDocument(d, [i]) for i, d in enumerate(tokenized_doc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38941"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_doc = len(tagged_data) #should be 38941\n",
    "num_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#settings to show epoch progress\n",
    "from gensim.test.utils import get_tmpfile\n",
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "\n",
    "class EpochSaver(CallbackAny2Vec):\n",
    "\n",
    "    def __init__(self, path_prefix):\n",
    "        self.path_prefix = path_prefix\n",
    "        self.epoch = 0\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        output_path = get_tmpfile('{}_epoch{}.model'.format(self.path_prefix, self.epoch))\n",
    "        model.save(output_path)\n",
    "        self.epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#settings to show epoch progress\n",
    "class EpochLogger(CallbackAny2Vec):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.epoch = 0\n",
    "        \n",
    "    def on_epoch_begin(self, model):\n",
    "        print(\"Epoch #{} start\".format(self.epoch))\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        print(\"Epoch #{} end\".format(self.epoch))\n",
    "        self.epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #0 start\n",
      "Epoch #0 end\n",
      "Epoch #1 start\n",
      "Epoch #1 end\n",
      "Epoch #2 start\n",
      "Epoch #2 end\n",
      "Epoch #3 start\n",
      "Epoch #3 end\n",
      "Epoch #4 start\n",
      "Epoch #4 end\n",
      "Epoch #5 start\n",
      "Epoch #5 end\n",
      "Epoch #6 start\n",
      "Epoch #6 end\n",
      "Epoch #7 start\n",
      "Epoch #7 end\n",
      "Epoch #8 start\n",
      "Epoch #8 end\n",
      "Epoch #9 start\n",
      "Epoch #9 end\n",
      "Epoch #10 start\n",
      "Epoch #10 end\n",
      "Epoch #11 start\n",
      "Epoch #11 end\n",
      "Epoch #12 start\n",
      "Epoch #12 end\n",
      "Epoch #13 start\n",
      "Epoch #13 end\n",
      "Epoch #14 start\n",
      "Epoch #14 end\n",
      "Epoch #15 start\n",
      "Epoch #15 end\n",
      "Epoch #16 start\n",
      "Epoch #16 end\n",
      "Epoch #17 start\n",
      "Epoch #17 end\n",
      "Epoch #18 start\n",
      "Epoch #18 end\n",
      "Epoch #19 start\n",
      "Epoch #19 end\n",
      "Epoch #20 start\n",
      "Epoch #20 end\n",
      "Epoch #21 start\n",
      "Epoch #21 end\n",
      "Epoch #22 start\n",
      "Epoch #22 end\n",
      "Epoch #23 start\n",
      "Epoch #23 end\n",
      "Epoch #24 start\n",
      "Epoch #24 end\n",
      "Epoch #25 start\n",
      "Epoch #25 end\n",
      "Epoch #26 start\n",
      "Epoch #26 end\n",
      "Epoch #27 start\n",
      "Epoch #27 end\n",
      "Epoch #28 start\n",
      "Epoch #28 end\n",
      "Epoch #29 start\n",
      "Epoch #29 end\n",
      "Epoch #30 start\n",
      "Epoch #30 end\n",
      "Epoch #31 start\n",
      "Epoch #31 end\n",
      "Epoch #32 start\n",
      "Epoch #32 end\n",
      "Epoch #33 start\n",
      "Epoch #33 end\n",
      "Epoch #34 start\n",
      "Epoch #34 end\n",
      "Epoch #35 start\n",
      "Epoch #35 end\n",
      "Epoch #36 start\n",
      "Epoch #36 end\n",
      "Epoch #37 start\n",
      "Epoch #37 end\n",
      "Epoch #38 start\n",
      "Epoch #38 end\n",
      "Epoch #39 start\n",
      "Epoch #39 end\n",
      "Epoch #40 start\n",
      "Epoch #40 end\n",
      "Epoch #41 start\n",
      "Epoch #41 end\n",
      "Epoch #42 start\n",
      "Epoch #42 end\n",
      "Epoch #43 start\n",
      "Epoch #43 end\n",
      "Epoch #44 start\n",
      "Epoch #44 end\n",
      "Epoch #45 start\n",
      "Epoch #45 end\n",
      "Epoch #46 start\n",
      "Epoch #46 end\n",
      "Epoch #47 start\n",
      "Epoch #47 end\n",
      "Epoch #48 start\n",
      "Epoch #48 end\n",
      "Epoch #49 start\n",
      "Epoch #49 end\n",
      "Epoch #50 start\n",
      "Epoch #50 end\n",
      "Epoch #51 start\n",
      "Epoch #51 end\n",
      "Epoch #52 start\n",
      "Epoch #52 end\n",
      "Epoch #53 start\n",
      "Epoch #53 end\n",
      "Epoch #54 start\n",
      "Epoch #54 end\n",
      "Epoch #55 start\n",
      "Epoch #55 end\n",
      "Epoch #56 start\n",
      "Epoch #56 end\n",
      "Epoch #57 start\n",
      "Epoch #57 end\n",
      "Epoch #58 start\n",
      "Epoch #58 end\n",
      "Epoch #59 start\n",
      "Epoch #59 end\n",
      "Epoch #60 start\n",
      "Epoch #60 end\n",
      "Epoch #61 start\n",
      "Epoch #61 end\n",
      "Epoch #62 start\n",
      "Epoch #62 end\n",
      "Epoch #63 start\n",
      "Epoch #63 end\n",
      "Epoch #64 start\n",
      "Epoch #64 end\n",
      "Epoch #65 start\n",
      "Epoch #65 end\n",
      "Epoch #66 start\n",
      "Epoch #66 end\n",
      "Epoch #67 start\n",
      "Epoch #67 end\n",
      "Epoch #68 start\n",
      "Epoch #68 end\n",
      "Epoch #69 start\n",
      "Epoch #69 end\n",
      "Epoch #70 start\n",
      "Epoch #70 end\n",
      "Epoch #71 start\n",
      "Epoch #71 end\n",
      "Epoch #72 start\n",
      "Epoch #72 end\n",
      "Epoch #73 start\n",
      "Epoch #73 end\n",
      "Epoch #74 start\n",
      "Epoch #74 end\n",
      "Epoch #75 start\n",
      "Epoch #75 end\n",
      "Epoch #76 start\n",
      "Epoch #76 end\n",
      "Epoch #77 start\n",
      "Epoch #77 end\n",
      "Epoch #78 start\n",
      "Epoch #78 end\n",
      "Epoch #79 start\n",
      "Epoch #79 end\n",
      "Epoch #80 start\n",
      "Epoch #80 end\n",
      "Epoch #81 start\n",
      "Epoch #81 end\n",
      "Epoch #82 start\n",
      "Epoch #82 end\n",
      "Epoch #83 start\n",
      "Epoch #83 end\n",
      "Epoch #84 start\n",
      "Epoch #84 end\n",
      "Epoch #85 start\n",
      "Epoch #85 end\n",
      "Epoch #86 start\n",
      "Epoch #86 end\n",
      "Epoch #87 start\n",
      "Epoch #87 end\n",
      "Epoch #88 start\n",
      "Epoch #88 end\n",
      "Epoch #89 start\n",
      "Epoch #89 end\n",
      "Epoch #90 start\n",
      "Epoch #90 end\n",
      "Epoch #91 start\n",
      "Epoch #91 end\n",
      "Epoch #92 start\n",
      "Epoch #92 end\n",
      "Epoch #93 start\n",
      "Epoch #93 end\n",
      "Epoch #94 start\n",
      "Epoch #94 end\n",
      "Epoch #95 start\n",
      "Epoch #95 end\n",
      "Epoch #96 start\n",
      "Epoch #96 end\n",
      "Epoch #97 start\n",
      "Epoch #97 end\n",
      "Epoch #98 start\n",
      "Epoch #98 end\n",
      "Epoch #99 start\n",
      "Epoch #99 end\n"
     ]
    }
   ],
   "source": [
    "epoch_logger = EpochLogger()\n",
    "## Train doc2vec model\n",
    "model = Doc2Vec(tagged_data, vector_size=20, window=2, min_count=1, workers=4, epochs = 100, callbacks=[epoch_logger])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trained doc2vec model\n",
    "model.save(\"my_doc2vec_new.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load saved doc2vec model\n",
    "model= Doc2Vec.load(\"my_doc2vec_new.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38941"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#confirm length (should be 38941)\n",
    "len(tokenized_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get vector value\n",
    "vec = np.empty([38941,20])\n",
    "\n",
    "for k,i in enumerate(tokenized_doc):\n",
    "    \n",
    "    #print(i)\n",
    "    vector = model.infer_vector(i)\n",
    "    vec[k] = vector\n",
    "    #vec = np.append(vector)\n",
    "    #vecf = np.append(vec,vector)\n",
    "\n",
    "# reshape into 2D\n",
    "new_arr = np.reshape(vec,(-1,20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = range(1, 21)\n",
    "vec_df = pd.DataFrame(new_arr, columns=['vec_' + str(i) for i in rng])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 38941 entries, 0 to 38940\n",
      "Data columns (total 20 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   vec_1   38941 non-null  float64\n",
      " 1   vec_2   38941 non-null  float64\n",
      " 2   vec_3   38941 non-null  float64\n",
      " 3   vec_4   38941 non-null  float64\n",
      " 4   vec_5   38941 non-null  float64\n",
      " 5   vec_6   38941 non-null  float64\n",
      " 6   vec_7   38941 non-null  float64\n",
      " 7   vec_8   38941 non-null  float64\n",
      " 8   vec_9   38941 non-null  float64\n",
      " 9   vec_10  38941 non-null  float64\n",
      " 10  vec_11  38941 non-null  float64\n",
      " 11  vec_12  38941 non-null  float64\n",
      " 12  vec_13  38941 non-null  float64\n",
      " 13  vec_14  38941 non-null  float64\n",
      " 14  vec_15  38941 non-null  float64\n",
      " 15  vec_16  38941 non-null  float64\n",
      " 16  vec_17  38941 non-null  float64\n",
      " 17  vec_18  38941 non-null  float64\n",
      " 18  vec_19  38941 non-null  float64\n",
      " 19  vec_20  38941 non-null  float64\n",
      "dtypes: float64(20)\n",
      "memory usage: 5.9 MB\n"
     ]
    }
   ],
   "source": [
    "#check vectors \n",
    "vec_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging vectors to main dataset (job)\n",
    "con_job = pd.concat([job, vec_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "con_job.to_csv('con_job_new.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "###sample with 10 rows\n",
    "\n",
    "# tokenized_doc = []\n",
    "# #doc = df_job_descriptions['jd_combo']\n",
    "# doc = docs_sample\n",
    "# for d in doc:\n",
    "#     tokenized_doc.append(preprocess(d))\n",
    "# #tokenized_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert tokenized document into gensim formated tagged data\n",
    "# tagged_data = [TaggedDocument(d, [i]) for i, d in enumerate(tokenized_doc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_doc = len(tagged_data)\n",
    "# num_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Get vector value\n",
    "# vec = np.empty([9,20])\n",
    "# for i in tokenized_doc:\n",
    "#     #print(i)\n",
    "#     vector = model.infer_vector(i)\n",
    "#     vecf = np.append(vec,vector)\n",
    "\n",
    "# # reshape into 2D\n",
    "# new_arr = np.reshape(vecf,(-1,20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rng = range(1, 21)\n",
    "# vec_df = pd.DataFrame(new_arr, columns=['vec_' + str(i) for i in rng])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
